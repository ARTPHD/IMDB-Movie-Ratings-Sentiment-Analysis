# -*- coding: utf-8 -*-
"""IMDb_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KhdtFiYBz8P6UALNDZwSGJ5pgxW14uxu
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.combine import SMOTETomek 
from imblearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report 
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import confusion_matrix, roc_auc_score
from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score
import pickle
from pickle import dump
from pickle import load

def normalization(text):
    # transform text into lowercase
    text = text.lower()
    
    # remove the punctuation and special characters 
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # remove digits
    text = re.sub(r'[\d+]', '', text)
    
    # remove greeks
    text = (re.sub('(?![ -~]).', '', text))
    
    return text
    
def tokenization(text):
    # split the text into a series of tokens
    text = word_tokenize(text)
    
    return text

def removedStopwords(text):
    
    # initialize an empty list of tokens with no stopwords
    tokens_with_no_stopwords = list()
    
    # extract tokens not found in a list of stopwords 
    for word in text:
        if word not in stopwords.words('english'):
            tokens_with_no_stopwords.append(word)
    
    return tokens_with_no_stopwords
    
def removed_words_less_than_4_characters(text):
    # initialize an empty list for tokens with more than 4 characters
    tokens_with_more_than_4_characters = list()
    
    # extract tokens with more than 4 characters
    for word in text:
        if len(word) >= 4:
            tokens_with_more_than_4_characters.append(word)
            
    return tokens_with_more_than_4_characters

def wordsLemmatization(text):
    # define object for lemmatizer 
    lemmatizer = WordNetLemmatizer()
    
    # initialize an empty list for lemmatized words
    wordsLemmatize = list()
    
    # lemmatize the words
    for word in text:
        wordsLemmatize.append(lemmatizer.lemmatize(word))
    
    return wordsLemmatize

def sentence_reconstruction(text):
    # initialize an emppty string 
    sentence_reconstruction = ""
    
    # combine each token to form a string 
    for word in text:
        sentence_reconstruction = sentence_reconstruction + word + " "
    return sentence_reconstruction

def text_subjectivity(text):
    text_ = TextBlob(text)
    return text_.sentiment.subjectivity

def text_polarity(text):
    text_ = TextBlob(text)
    return text_.sentiment.polarity

def sentiment_analysis_outcome(polarity):
    if polarity > 0:
        return "Positive"
    else:
        return "Negative"

df = pd.read_csv('movie.csv')
df.head(10)

# Checking the size of dataset
df.shape

#Summarizing the overall dataset
df.info()

#Checking the null values in dataset
df.isnull().sum()

sns.heatmap(df.isnull(), annot = False, cmap = 'coolwarm')

df.dropna(axis = 0, how = 'any', inplace = True)

#Checking the null values in dataset
df.isnull().sum()

df['text'] = df['text'].apply(normalization)

df['text'] = df['text'].apply(tokenization)

df['text'] = df['text'].apply(removedStopwords)

df['text'] = df['text'].apply(removed_words_less_than_4_characters)

df['text'] = df['text'].apply(wordsLemmatization)

df['text'] = df['text'].apply(sentence_reconstruction)

df_sujectivity = df['text'].apply(text_subjectivity)
# create a new column in dataframe for subjectivity
df['subjectivity'] = df_sujectivity

df_polarity = df['text'].apply(text_polarity)
# create a new column in dataframe for polarity
df['polarity'] = df_polarity

df['sentiment_analysis'] = df_polarity.apply(sentiment_analysis_outcome)

df['sentiment_analysis'].value_counts()

sns.set_style("darkgrid")
sns.scatterplot(x = df_polarity, y = df_sujectivity)
plt.title("Sentiment Analysis of Polarity vs Subjectivity", fontweight = 'bold', fontsize = 15)
plt.xlabel("Polarity", fontweight = 'bold', fontsize = 12)
plt.ylabel("Subjectivity", fontweight = 'bold', fontsize = 12)
plt.show()

sns.set_style("darkgrid")
df['sentiment_analysis'].value_counts().plot(kind = "bar", color = ["crimson", "navy"])
plt.title("Distribution of Sentiment Analysis Outcome", fontsize = 15, fontweight = 'bold')
plt.xlabel("Sentiment Analysis Outcome", fontsize = 12, fontweight = 'bold')
plt.ylabel("Number of Observations", fontsize = 12, fontweight = 'bold')
plt.show()

"""### **Modelling**"""

#Melakukan spliting untuk X dan Y pada training dan testing
#X merupakan  Column text dan Y merupakan Column sentiment_analysis
X_train, X_test, y_train, y_test = train_test_split(df['text'],
                                                    df['sentiment_analysis'],
                                                    test_size = 0.25, 
                                                    random_state = 42)

#Menggunakan TfidfVectorizer untuk Mengonversi kumpulan dokumen mentah menjadi matriks fitur TF-IDF.
#Menggunakan SMOTETomek untuk Gabungkan pengambilan sampel berlebih dan kurang menggunakan tautan SMOTE dan Tomek.
#Model SVM menggunakan kerner linear random_state bernilai 42 yang banyak digunakan dalam banyak contoh resmi scikit 
steps = [('tfidfVectorizer', TfidfVectorizer(analyzer = 'word')), 
             ('smt', SMOTETomek(random_state = 42)),
             ('svm', SVC(kernel = "linear", random_state = 42))]

#Membuat Pipeline untuk menampung transformator dan prediktor di dalam steps
model_svm = Pipeline(steps = steps)

#Train model_svm
model_svm.fit(X_train, y_train)

# Memprediksi label set train dan test
y_pred_train_svm = model_svm.predict(X_train)
y_pred_test_svm = model_svm.predict(X_test)

#Menampilkan report dari training set dan testing set
print("Classification Report for Support Vector Machine (SVM) with Training Set: ")
print(classification_report(y_train, y_pred_train_svm))
print()
print("Classification Report for Support Vector Machine (SVM) with Testing Set:")
print(classification_report(y_test, y_pred_test_svm))

#Menampilkan Confusion_Matrix dari Training set
conf_matrix = confusion_matrix(y_train, y_pred_train_svm, labels = ['Negative', 'Positive'])
sns.heatmap(conf_matrix, annot = True, cmap = 'viridis')
plt.title("Confusion Matrix for Support Vector Machine (SVM) (Training Set)", fontweight = 'bold', fontsize = 12)
plt.xlabel("Actual", fontweight = 'bold', fontsize = 12)
plt.ylabel("Predicted", fontweight = 'bold', fontsize = 12)

#Menampilkan Confusion_Matrix dari Testing set
conf_matrix = confusion_matrix(y_test, y_pred_test_svm, labels = ['Negative', 'Positive'])
sns.heatmap(conf_matrix, annot = True, cmap = 'viridis')
plt.title("Confusion Matrix for Support Vector Machine (SVM) (Testing Set)", fontweight = 'bold', fontsize = 12)
plt.xlabel("Actual", fontweight = 'bold', fontsize = 12)
plt.ylabel("Predicted", fontweight = 'bold', fontsize = 12)

#Menampilkan Perfomance Matrics yang telah digambarkan pada Confusion Matrix Training set dan Testing set diatas
print("Performance Metrics for Support Vector Machine (SVM) (Training Set):")
print("Accuracy Score: ", accuracy_score(y_train, y_pred_train_svm))
print("Precision Score: ", precision_score(y_train, y_pred_train_svm, average = 'weighted'))
print("F1 Score: ", f1_score(y_train, y_pred_train_svm, average = "weighted"))
print("Recall Score: ", recall_score(y_train, y_pred_train_svm, average = "weighted"))
print()
print("Performance Metrics for Support Vector Machine (SVM) (Testing Set):")
print("Accuracy Score: ", accuracy_score(y_test, y_pred_test_svm))
print("Precision Score: ", precision_score(y_test, y_pred_test_svm, average = 'weighted'))
print("F1 Score: ", f1_score(y_test, y_pred_test_svm, average = "weighted"))
print("Recall Score: ", recall_score(y_test, y_pred_test_svm, average = "weighted"))

#Saving the model to file
dump(model_svm, open('IMDb_svm.pkl', 'wb'))

#Loading the file
ml = load(open('IMDb_svm.pkl', 'rb'))